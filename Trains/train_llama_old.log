Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Unsloth: Will map <|im_end|> to EOS = </s>.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Collecting wandb
  Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Collecting click!=8.0.0,>=7.1 (from wandb)
  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)
Collecting docker-pycreds>=0.4.0 (from wandb)
  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)
Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)
  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)
Collecting platformdirs (from wandb)
  Downloading platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)
Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)
  Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
Collecting psutil>=5.0.0 (from wandb)
  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting pydantic<3 (from wandb)
  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)
Collecting pyyaml (from wandb)
  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting requests<3,>=2.0.0 (from wandb)
  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting sentry-sdk>=2.0.0 (from wandb)
  Downloading sentry_sdk-2.28.0-py2.py3-none-any.whl.metadata (10 kB)
Collecting setproctitle (from wandb)
  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Collecting setuptools (from wandb)
  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)
Collecting typing-extensions<5,>=4.4 (from wandb)
  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)
Collecting six>=1.4.0 (from docker-pycreds>=0.4.0->wandb)
  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)
  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)
Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)
  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)
  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)
  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)
Collecting charset-normalizer<4,>=2 (from requests<3,>=2.0.0->wandb)
  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)
Collecting idna<4,>=2.5 (from requests<3,>=2.0.0->wandb)
  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.0.0->wandb)
  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi>=2017.4.17 (from requests<3,>=2.0.0->wandb)
  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)
Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)
  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)
Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/21.4 MB 420.1 MB/s eta 0:00:00
Downloading click-8.2.0-py3-none-any.whl (102 kB)
Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)
Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)
Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl (320 kB)
Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)
Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 437.6 MB/s eta 0:00:00
Downloading requests-2.32.3-py3-none-any.whl (64 kB)
Downloading sentry_sdk-2.28.0-py2.py3-none-any.whl (341 kB)
Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)
Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)
Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 751.2/751.2 kB 408.8 MB/s eta 0:00:00
Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)
Downloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 430.2 MB/s eta 0:00:00
Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)
Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)
Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)
Downloading idna-3.10-py3-none-any.whl (70 kB)
Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)
Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)
Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)
Downloading smmap-5.0.2-py3-none-any.whl (24 kB)
Installing collected packages: urllib3, typing-extensions, smmap, six, setuptools, setproctitle, pyyaml, psutil, protobuf, platformdirs, idna, click, charset-normalizer, certifi, annotated-types, typing-inspection, sentry-sdk, requests, pydantic-core, gitdb, docker-pycreds, pydantic, gitpython, wandb
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.21.0 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.
unsloth 2025.3.19 requires protobuf<4.0.0, but you have protobuf 6.31.0 which is incompatible.
unsloth 2025.3.19 requires torch>=2.4.0, but you have torch 2.2.2+cpu which is incompatible.
unsloth-zoo 2025.3.17 requires protobuf<4.0.0, but you have protobuf 6.31.0 which is incompatible.
xformers 0.0.29.post3 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.
Successfully installed annotated-types-0.7.0 certifi-2025.4.26 charset-normalizer-3.4.2 click-8.2.0 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 idna-3.10 platformdirs-4.3.8 protobuf-6.31.0 psutil-7.0.0 pydantic-2.11.4 pydantic-core-2.33.2 pyyaml-6.0.2 requests-2.32.3 sentry-sdk-2.28.0 setproctitle-1.3.6 setuptools-80.7.1 six-1.17.0 smmap-5.0.2 typing-extensions-4.13.2 typing-inspection-0.4.0 urllib3-2.4.0 wandb-0.19.11
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/wandb-0.19.11.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/click-8.2.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/platformdirs-4.3.8.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/typing_inspection-0.4.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/requests already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/google already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/smmap already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/sentry_sdk-2.28.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/setproctitle-1.3.6.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/gitdb-4.0.12.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/platformdirs already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/setuptools already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/psutil already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/certifi-2025.4.26.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/setproctitle already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pydantic-2.11.4.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/package_readme.md already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/PyYAML-6.0.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/urllib3 already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/six.py already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/typing_extensions.py already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/GitPython-3.1.44.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/smmap-5.0.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/git already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/charset_normalizer-3.4.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/gitdb already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/psutil-7.0.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/certifi already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/annotated_types already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/_distutils_hack already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/docker_pycreds-0.4.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/typing_inspection already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pydantic already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/yaml already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/distutils-precedence.pth already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/annotated_types-0.7.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/sentry_sdk already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pydantic_core already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/urllib3-2.4.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/requests-2.32.3.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pkg_resources already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/dockerpycreds already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pydantic_core-2.33.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/_yaml already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/click already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/charset_normalizer already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/wandb already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/idna already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/typing_extensions-4.13.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/bin already exists. Specify --upgrade to force replacement.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: tijaniyunus07 (tijaniyunus07-constructor-institute) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 395 | Num Epochs = 2 | Total steps = 98
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 41,943,040/7,000,000,000 (0.60% trained)
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/coder/project/yunus/wandb/run-20250518_113200-731dfhwc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run yunus_mistral_finetune
wandb: ⭐️ View project at https://wandb.ai/tijaniyunus07-constructor-institute/huggingface
wandb: 🚀 View run at https://wandb.ai/tijaniyunus07-constructor-institute/huggingface/runs/731dfhwc
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.50.3.
   \\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.138 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
GPU = NVIDIA A100 80GB PCIe. Max memory = 79.138 GB.
4.051 GB of memory reserved.
  0%|          | 0/98 [00:00<?, ?it/s]  1%|          | 1/98 [00:06<09:42,  6.01s/it]  2%|▏         | 2/98 [00:07<05:40,  3.55s/it]  3%|▎         | 3/98 [00:10<04:39,  2.94s/it]  4%|▍         | 4/98 [00:12<04:00,  2.56s/it]  5%|▌         | 5/98 [00:14<03:39,  2.36s/it]                                                5%|▌         | 5/98 [00:14<03:39,  2.36s/it]  6%|▌         | 6/98 [00:15<03:20,  2.18s/it]  7%|▋         | 7/98 [00:17<03:10,  2.09s/it]  8%|▊         | 8/98 [00:19<03:06,  2.07s/it]  9%|▉         | 9/98 [00:21<02:59,  2.02s/it] 10%|█         | 10/98 [00:23<02:52,  1.96s/it]                                                10%|█         | 10/98 [00:23<02:52,  1.96s/it] 11%|█         | 11/98 [00:26<03:04,  2.13s/it] 12%|█▏        | 12/98 [00:27<02:52,  2.01s/it] 13%|█▎        | 13/98 [00:29<02:47,  1.97s/it] 14%|█▍        | 14/98 [00:31<02:42,  1.94s/it] 15%|█▌        | 15/98 [00:33<02:46,  2.01s/it]                                                15%|█▌        | 15/98 [00:33<02:46,  2.01s/it] 16%|█▋        | 16/98 [00:35<02:39,  1.95s/it] 17%|█▋        | 17/98 [00:37<02:39,  1.97s/it] 18%|█▊        | 18/98 [00:39<02:34,  1.93s/it] 19%|█▉        | 19/98 [00:41<02:33,  1.95s/it] 20%|██        | 20/98 [00:43<02:28,  1.90s/it]                                                20%|██        | 20/98 [00:43<02:28,  1.90s/it] 21%|██▏       | 21/98 [00:44<02:22,  1.85s/it] 22%|██▏       | 22/98 [00:46<02:20,  1.85s/it] 23%|██▎       | 23/98 [00:48<02:22,  1.90s/it] 24%|██▍       | 24/98 [00:50<02:22,  1.93s/it] 26%|██▌       | 25/98 [00:52<02:16,  1.87s/it]                                                26%|██▌       | 25/98 [00:52<02:16,  1.87s/it]Unsloth: Not an error, but MistralForCausalLM does not accept `num_items_in_batch`.
Using gradient accumulation will be very slightly less accurate.
Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 2.3453, 'grad_norm': 2.1075284481048584, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 1.6129, 'grad_norm': 0.9683259725570679, 'learning_rate': 0.000189247311827957, 'epoch': 0.2}
{'loss': 1.3963, 'grad_norm': 0.8083158731460571, 'learning_rate': 0.00017849462365591398, 'epoch': 0.3}
{'loss': 1.3476, 'grad_norm': 1.2174057960510254, 'learning_rate': 0.00016774193548387098, 'epoch': 0.4}
{'loss': 1.3843, 'grad_norm': 1.3618625402450562, 'learning_rate': 0.00015698924731182796, 'epoch': 0.51}

  0%|          | 0/22 [00:00<?, ?it/s][A
 14%|█▎        | 3/22 [00:00<00:01, 15.50it/s][A
 23%|██▎       | 5/22 [00:00<00:01, 10.36it/s][A
 32%|███▏      | 7/22 [00:00<00:01,  8.88it/s][A
 36%|███▋      | 8/22 [00:01<00:02,  6.27it/s][A
 41%|████      | 9/22 [00:01<00:01,  6.89it/s][A
 50%|█████     | 11/22 [00:01<00:01,  7.90it/s][A
 59%|█████▉    | 13/22 [00:01<00:01,  8.72it/s][A
 64%|██████▎   | 14/22 [00:01<00:01,  7.43it/s][A
 73%|███████▎  | 16/22 [00:01<00:00,  8.34it/s][A
 82%|████████▏ | 18/22 [00:02<00:00,  8.86it/s][A
 91%|█████████ | 20/22 [00:02<00:00,  9.33it/s][A
100%|██████████| 22/22 [00:02<00:00,  9.74it/s][A                                               
                                               [A 26%|██▌       | 25/98 [00:55<02:16,  1.87s/it]
100%|██████████| 22/22 [00:02<00:00,  9.74it/s][A
                                               [A 27%|██▋       | 26/98 [00:56<03:11,  2.66s/it] 28%|██▊       | 27/98 [00:58<02:51,  2.42s/it] 29%|██▊       | 28/98 [01:00<02:36,  2.24s/it] 30%|██▉       | 29/98 [01:02<02:23,  2.08s/it] 31%|███       | 30/98 [01:04<02:19,  2.05s/it]                                                31%|███       | 30/98 [01:04<02:19,  2.05s/it] 32%|███▏      | 31/98 [01:06<02:11,  1.96s/it] 33%|███▎      | 32/98 [01:08<02:13,  2.02s/it] 34%|███▎      | 33/98 [01:10<02:09,  2.00s/it] 35%|███▍      | 34/98 [01:11<02:04,  1.94s/it] 36%|███▌      | 35/98 [01:13<01:58,  1.88s/it]                                                36%|███▌      | 35/98 [01:13<01:58,  1.88s/it] 37%|███▋      | 36/98 [01:15<01:54,  1.84s/it] 38%|███▊      | 37/98 [01:17<01:50,  1.81s/it] 39%|███▉      | 38/98 [01:19<01:49,  1.82s/it] 40%|███▉      | 39/98 [01:20<01:45,  1.79s/it] 41%|████      | 40/98 [01:22<01:43,  1.78s/it]                                                41%|████      | 40/98 [01:22<01:43,  1.78s/it] 42%|████▏     | 41/98 [01:24<01:41,  1.78s/it] 43%|████▎     | 42/98 [01:26<01:42,  1.83s/it] 44%|████▍     | 43/98 [01:28<01:43,  1.88s/it] 45%|████▍     | 44/98 [01:30<01:40,  1.86s/it] 46%|████▌     | 45/98 [01:31<01:37,  1.84s/it]                                                46%|████▌     | 45/98 [01:31<01:37,  1.84s/it] 47%|████▋     | 46/98 [01:33<01:34,  1.82s/it] 48%|████▊     | 47/98 [01:35<01:38,  1.92s/it] 49%|████▉     | 48/98 [01:37<01:40,  2.01s/it] 50%|█████     | 49/98 [01:39<01:37,  2.00s/it] 51%|█████     | 50/98 [01:41<01:22,  1.71s/it]                                                51%|█████     | 50/98 [01:41<01:22,  1.71s/it]{'eval_loss': 1.3656643629074097, 'eval_runtime': 2.7082, 'eval_samples_per_second': 16.247, 'eval_steps_per_second': 8.123, 'epoch': 0.51}
{'loss': 1.3822, 'grad_norm': 0.7438440918922424, 'learning_rate': 0.00014623655913978496, 'epoch': 0.61}
{'loss': 1.298, 'grad_norm': 0.9703931212425232, 'learning_rate': 0.00013548387096774193, 'epoch': 0.71}
{'loss': 1.265, 'grad_norm': 1.0934420824050903, 'learning_rate': 0.00012473118279569893, 'epoch': 0.81}
{'loss': 1.289, 'grad_norm': 1.5490418672561646, 'learning_rate': 0.00011397849462365593, 'epoch': 0.91}
{'loss': 1.199, 'grad_norm': 0.9716708660125732, 'learning_rate': 0.0001032258064516129, 'epoch': 1.0}

  0%|          | 0/22 [00:00<?, ?it/s][A
  9%|▉         | 2/22 [00:00<00:01, 19.89it/s][A
 18%|█▊        | 4/22 [00:00<00:01, 12.62it/s][A
 27%|██▋       | 6/22 [00:00<00:01,  8.57it/s][A
 36%|███▋      | 8/22 [00:01<00:02,  6.50it/s][A
 41%|████      | 9/22 [00:01<00:01,  7.00it/s][A
 50%|█████     | 11/22 [00:01<00:01,  7.99it/s][A
 59%|█████▉    | 13/22 [00:01<00:01,  8.60it/s][A
 64%|██████▎   | 14/22 [00:01<00:01,  7.36it/s][A
 68%|██████▊   | 15/22 [00:01<00:00,  7.63it/s][A
 77%|███████▋  | 17/22 [00:02<00:00,  8.41it/s][A
 86%|████████▋ | 19/22 [00:02<00:00,  9.01it/s][A
 95%|█████████▌| 21/22 [00:02<00:00,  9.39it/s][A                                               
                                               [A 51%|█████     | 50/98 [01:43<01:22,  1.71s/it]
100%|██████████| 22/22 [00:02<00:00,  9.39it/s][A
                                               [A 52%|█████▏    | 51/98 [01:45<01:59,  2.55s/it] 53%|█████▎    | 52/98 [01:47<01:50,  2.41s/it] 54%|█████▍    | 53/98 [01:49<01:40,  2.24s/it] 55%|█████▌    | 54/98 [01:51<01:35,  2.16s/it] 56%|█████▌    | 55/98 [01:53<01:29,  2.09s/it]                                                56%|█████▌    | 55/98 [01:53<01:29,  2.09s/it] 57%|█████▋    | 56/98 [01:55<01:25,  2.04s/it] 58%|█████▊    | 57/98 [01:57<01:20,  1.97s/it] 59%|█████▉    | 58/98 [01:59<01:20,  2.01s/it] 60%|██████    | 59/98 [02:00<01:15,  1.93s/it] 61%|██████    | 60/98 [02:03<01:15,  1.99s/it]                                                61%|██████    | 60/98 [02:03<01:15,  1.99s/it] 62%|██████▏   | 61/98 [02:05<01:13,  1.99s/it] 63%|██████▎   | 62/98 [02:06<01:09,  1.93s/it] 64%|██████▍   | 63/98 [02:08<01:06,  1.89s/it] 65%|██████▌   | 64/98 [02:10<01:05,  1.93s/it] 66%|██████▋   | 65/98 [02:12<01:04,  1.95s/it]                                                66%|██████▋   | 65/98 [02:12<01:04,  1.95s/it] 67%|██████▋   | 66/98 [02:14<01:02,  1.97s/it] 68%|██████▊   | 67/98 [02:16<00:58,  1.89s/it] 69%|██████▉   | 68/98 [02:18<00:55,  1.84s/it] 70%|███████   | 69/98 [02:19<00:52,  1.81s/it] 71%|███████▏  | 70/98 [02:21<00:51,  1.85s/it]                                                71%|███████▏  | 70/98 [02:21<00:51,  1.85s/it] 72%|███████▏  | 71/98 [02:23<00:50,  1.89s/it] 73%|███████▎  | 72/98 [02:25<00:51,  1.98s/it] 74%|███████▍  | 73/98 [02:27<00:47,  1.91s/it] 76%|███████▌  | 74/98 [02:29<00:44,  1.86s/it] 77%|███████▋  | 75/98 [02:31<00:43,  1.87s/it]                                                77%|███████▋  | 75/98 [02:31<00:43,  1.87s/it]{'eval_loss': 1.2460821866989136, 'eval_runtime': 2.6745, 'eval_samples_per_second': 16.451, 'eval_steps_per_second': 8.226, 'epoch': 1.0}
{'loss': 1.0681, 'grad_norm': 0.8357863426208496, 'learning_rate': 9.247311827956989e-05, 'epoch': 1.1}
{'loss': 0.9443, 'grad_norm': 0.6682923436164856, 'learning_rate': 8.172043010752689e-05, 'epoch': 1.2}
{'loss': 0.9949, 'grad_norm': 0.8031797409057617, 'learning_rate': 7.096774193548388e-05, 'epoch': 1.3}
{'loss': 0.9124, 'grad_norm': 1.0557438135147095, 'learning_rate': 6.021505376344086e-05, 'epoch': 1.4}
{'loss': 0.9658, 'grad_norm': 1.2115176916122437, 'learning_rate': 4.9462365591397855e-05, 'epoch': 1.51}

  0%|          | 0/22 [00:00<?, ?it/s][A
 14%|█▎        | 3/22 [00:00<00:01, 15.31it/s][A
 23%|██▎       | 5/22 [00:00<00:01, 10.35it/s][A
 32%|███▏      | 7/22 [00:00<00:01,  8.94it/s][A
 36%|███▋      | 8/22 [00:01<00:02,  6.28it/s][A
 41%|████      | 9/22 [00:01<00:01,  6.83it/s][A
 50%|█████     | 11/22 [00:01<00:01,  8.02it/s][A
 59%|█████▉    | 13/22 [00:01<00:01,  8.81it/s][A
 64%|██████▎   | 14/22 [00:01<00:01,  7.44it/s][A
 68%|██████▊   | 15/22 [00:01<00:00,  7.77it/s][A
 73%|███████▎  | 16/22 [00:01<00:00,  8.17it/s][A
 77%|███████▋  | 17/22 [00:02<00:00,  8.44it/s][A
 82%|████████▏ | 18/22 [00:02<00:00,  8.28it/s][A
 86%|████████▋ | 19/22 [00:02<00:00,  8.41it/s][A
 91%|█████████ | 20/22 [00:02<00:00,  8.60it/s][A
 95%|█████████▌| 21/22 [00:02<00:00,  8.60it/s][A                                               
                                               [A 77%|███████▋  | 75/98 [02:34<00:43,  1.87s/it]
100%|██████████| 22/22 [00:02<00:00,  8.60it/s][A
                                               [A 78%|███████▊  | 76/98 [02:35<00:58,  2.66s/it] 79%|███████▊  | 77/98 [02:37<00:50,  2.40s/it] 80%|███████▉  | 78/98 [02:39<00:43,  2.20s/it] 81%|████████  | 79/98 [02:41<00:39,  2.10s/it] 82%|████████▏ | 80/98 [02:43<00:36,  2.01s/it]                                                82%|████████▏ | 80/98 [02:43<00:36,  2.01s/it] 83%|████████▎ | 81/98 [02:44<00:33,  2.00s/it] 84%|████████▎ | 82/98 [02:46<00:31,  2.00s/it] 85%|████████▍ | 83/98 [02:48<00:28,  1.92s/it] 86%|████████▌ | 84/98 [02:50<00:26,  1.87s/it] 87%|████████▋ | 85/98 [02:52<00:24,  1.85s/it]                                                87%|████████▋ | 85/98 [02:52<00:24,  1.85s/it] 88%|████████▊ | 86/98 [02:54<00:22,  1.89s/it] 89%|████████▉ | 87/98 [02:56<00:21,  1.93s/it] 90%|████████▉ | 88/98 [02:58<00:19,  1.91s/it] 91%|█████████ | 89/98 [02:59<00:16,  1.85s/it] 92%|█████████▏| 90/98 [03:01<00:15,  1.89s/it]                                                92%|█████████▏| 90/98 [03:01<00:15,  1.89s/it] 93%|█████████▎| 91/98 [03:03<00:13,  1.86s/it] 94%|█████████▍| 92/98 [03:05<00:10,  1.82s/it] 95%|█████████▍| 93/98 [03:07<00:08,  1.79s/it] 96%|█████████▌| 94/98 [03:08<00:07,  1.76s/it] 97%|█████████▋| 95/98 [03:10<00:05,  1.81s/it]                                                97%|█████████▋| 95/98 [03:10<00:05,  1.81s/it] 98%|█████████▊| 96/98 [03:12<00:03,  1.91s/it] 99%|█████████▉| 97/98 [03:15<00:02,  2.03s/it]100%|██████████| 98/98 [03:16<00:00,  1.96s/it]                                               100%|██████████| 98/98 [03:17<00:00,  1.96s/it]100%|██████████| 98/98 [03:17<00:00,  2.02s/it]
Unsloth: Will map <|im_end|> to EOS = <|im_end|>.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'eval_loss': 1.218530535697937, 'eval_runtime': 2.7397, 'eval_samples_per_second': 16.06, 'eval_steps_per_second': 8.03, 'epoch': 1.51}
{'loss': 0.9806, 'grad_norm': 1.9646860361099243, 'learning_rate': 3.870967741935484e-05, 'epoch': 1.61}
{'loss': 0.9782, 'grad_norm': 1.3090457916259766, 'learning_rate': 2.7956989247311828e-05, 'epoch': 1.71}
{'loss': 0.8808, 'grad_norm': 0.9695473313331604, 'learning_rate': 1.7204301075268818e-05, 'epoch': 1.81}
{'loss': 0.9792, 'grad_norm': 1.3002984523773193, 'learning_rate': 6.451612903225806e-06, 'epoch': 1.91}
{'train_runtime': 198.6856, 'train_samples_per_second': 3.976, 'train_steps_per_second': 0.493, 'train_loss': 1.2135010203536676, 'epoch': 1.97}
198.6856 seconds used for training.
3.31 minutes used for training.
Peak reserved memory = 5.588 GB.
Peak reserved memory for training = 1.537 GB.
Peak reserved memory % of max memory = 7.061 %.
Peak reserved memory for training % of max memory = 1.942 %.
<|im_start|>user
Who is the coordinator of capstone course<|im_end|>
<|im_start|>assistant
The coordinator of the Capstone course is Dr. Ahmed Elmaghraby.<|im_end|>
<|im_start|>user
What is a famous tall tower in Paris?<|im_end|>
<|im_start|>assistant
The Eiffel Tower is a well-known landmark in Paris, France. It was built by Gustave Eiffel and is the tallest structure in the city.<|im_end|>
Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 215.31 out of 314.69 RAM for saving.
Unsloth: Saving model... This might take 5 minutes ...
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:00<00:00, 35.25it/s] 34%|███▍      | 11/32 [00:00<00:00, 54.28it/s] 59%|█████▉    | 19/32 [00:00<00:00, 63.02it/s] 84%|████████▍ | 27/32 [00:00<00:00, 67.05it/s]100%|██████████| 32/32 [00:00<00:00, 63.61it/s]
Unsloth: Saving tokenizer... Done.
Traceback (most recent call last):
  File "/home/coder/project/yunus/mistral_v0_3_(7b)_conversational.py", line 401, in <module>
    model.save_pretrained_merged("merged_model", tokenizer, save_method = "merged_16bit",)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/unsloth/save.py", line 1313, in unsloth_save_pretrained_merged
    unsloth_save_model(**arguments)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/unsloth/save.py", line 721, in unsloth_save_model
    internal_model.save_pretrained(**save_pretrained_settings)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/modeling_utils.py", line 3578, in save_pretrained
    safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/safetensors/torch.py", line 286, in save_file
    serialize_file(_flatten(tensors), filename, metadata=metadata)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/safetensors/torch.py", line 496, in _flatten
    return {
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/safetensors/torch.py", line 500, in <dictcomp>
    "data": _tobytes(v, k),
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/safetensors/torch.py", line 460, in _tobytes
    return data.tobytes()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/coder/project/yunus/mistral_v0_3_(7b)_conversational.py", line 401, in <module>
    model.save_pretrained_merged("merged_model", tokenizer, save_method = "merged_16bit",)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/unsloth/save.py", line 1313, in unsloth_save_pretrained_merged
    unsloth_save_model(**arguments)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/unsloth/save.py", line 721, in unsloth_save_model
    internal_model.save_pretrained(**save_pretrained_settings)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/modeling_utils.py", line 3578, in save_pretrained
    safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/safetensors/torch.py", line 286, in save_file
    serialize_file(_flatten(tensors), filename, metadata=metadata)
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/safetensors/torch.py", line 496, in _flatten
    return {
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/safetensors/torch.py", line 500, in <dictcomp>
    "data": _tobytes(v, k),
  File "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/safetensors/torch.py", line 460, in _tobytes
    return data.tobytes()
KeyboardInterrupt
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33myunus_mistral_finetune[0m at: [34mhttps://wandb.ai/tijaniyunus07-constructor-institute/huggingface/runs/731dfhwc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250518_113200-731dfhwc/logs[0m
