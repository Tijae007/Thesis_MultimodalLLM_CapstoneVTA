Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Unsloth: Will map <|im_end|> to EOS = </s>.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.50.3.
   \\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.138 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 439 examples [00:00, 28296.78 examples/s]
Map:   0%|          | 0/395 [00:00<?, ? examples/s]Map: 100%|██████████| 395/395 [00:00<00:00, 5758.80 examples/s]
Map:   0%|          | 0/44 [00:00<?, ? examples/s]Map: 100%|██████████| 44/44 [00:00<00:00, 3508.61 examples/s]
Collecting wandb
  Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Collecting click!=8.0.0,>=7.1 (from wandb)
  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)
Collecting docker-pycreds>=0.4.0 (from wandb)
  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)
Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)
  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)
Collecting platformdirs (from wandb)
  Downloading platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)
Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)
  Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
Collecting psutil>=5.0.0 (from wandb)
  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting pydantic<3 (from wandb)
  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)
Collecting pyyaml (from wandb)
  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting requests<3,>=2.0.0 (from wandb)
  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting sentry-sdk>=2.0.0 (from wandb)
  Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl.metadata (10 kB)
Collecting setproctitle (from wandb)
  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Collecting setuptools (from wandb)
  Downloading setuptools-80.8.0-py3-none-any.whl.metadata (6.6 kB)
Collecting typing-extensions<5,>=4.4 (from wandb)
  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)
Collecting six>=1.4.0 (from docker-pycreds>=0.4.0->wandb)
  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)
  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)
Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)
  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)
  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)
  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)
Collecting charset-normalizer<4,>=2 (from requests<3,>=2.0.0->wandb)
  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)
Collecting idna<4,>=2.5 (from requests<3,>=2.0.0->wandb)
  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.0.0->wandb)
  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi>=2017.4.17 (from requests<3,>=2.0.0->wandb)
  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)
Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)
  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)
Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/21.4 MB 302.8 MB/s eta 0:00:00
Downloading click-8.2.1-py3-none-any.whl (102 kB)
Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)
Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)
Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl (320 kB)
Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)
Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 614.4 MB/s eta 0:00:00
Downloading requests-2.32.3-py3-none-any.whl (64 kB)
Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl (341 kB)
Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)
Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)
Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 751.2/751.2 kB 517.1 MB/s eta 0:00:00
Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)
Downloading setuptools-80.8.0-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 567.0 MB/s eta 0:00:00
Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)
Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)
Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)
Downloading idna-3.10-py3-none-any.whl (70 kB)
Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)
Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)
Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)
Downloading smmap-5.0.2-py3-none-any.whl (24 kB)
Installing collected packages: urllib3, typing-extensions, smmap, six, setuptools, setproctitle, pyyaml, psutil, protobuf, platformdirs, idna, click, charset-normalizer, certifi, annotated-types, typing-inspection, sentry-sdk, requests, pydantic-core, gitdb, docker-pycreds, pydantic, gitpython, wandb
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.21.0 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.
unsloth 2025.3.19 requires protobuf<4.0.0, but you have protobuf 6.31.0 which is incompatible.
unsloth 2025.3.19 requires torch>=2.4.0, but you have torch 2.2.2+cpu which is incompatible.
unsloth-zoo 2025.3.17 requires protobuf<4.0.0, but you have protobuf 6.31.0 which is incompatible.
xformers 0.0.29.post3 requires torch==2.6.0, but you have torch 2.2.2+cpu which is incompatible.
Successfully installed annotated-types-0.7.0 certifi-2025.4.26 charset-normalizer-3.4.2 click-8.2.1 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 idna-3.10 platformdirs-4.3.8 protobuf-6.31.0 psutil-7.0.0 pydantic-2.11.4 pydantic-core-2.33.2 pyyaml-6.0.2 requests-2.32.3 sentry-sdk-2.29.1 setproctitle-1.3.6 setuptools-80.8.0 six-1.17.0 smmap-5.0.2 typing-extensions-4.13.2 typing-inspection-0.4.1 urllib3-2.4.0 wandb-0.19.11
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/wandb-0.19.11.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/platformdirs-4.3.8.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/click-8.2.1.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/requests already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/google already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/smmap already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/setproctitle-1.3.6.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/gitdb-4.0.12.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/platformdirs already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/protobuf-6.31.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/setuptools already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/psutil already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/certifi-2025.4.26.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/setproctitle already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pydantic-2.11.4.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/package_readme.md already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/PyYAML-6.0.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/urllib3 already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/six.py already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/typing_extensions.py already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/GitPython-3.1.44.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/smmap-5.0.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/git already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/charset_normalizer-3.4.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/gitdb already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/psutil-7.0.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/certifi already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/annotated_types already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/_distutils_hack already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/docker_pycreds-0.4.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/sentry_sdk-2.29.1.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/typing_inspection already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pydantic already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/yaml already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/distutils-precedence.pth already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/annotated_types-0.7.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/sentry_sdk already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pydantic_core already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/urllib3-2.4.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/requests-2.32.3.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pkg_resources already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/setuptools-80.8.0.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/dockerpycreds already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/pydantic_core-2.33.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/typing_inspection-0.4.1.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/_yaml already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/click already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/charset_normalizer already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/wandb already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/idna already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/typing_extensions-4.13.2.dist-info already exists. Specify --upgrade to force replacement.
WARNING: Target directory /home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/bin already exists. Specify --upgrade to force replacement.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: tijaniyunus07 (tijaniyunus07-constructor-institute) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Unsloth: Tokenizing ["text"] (num_proc=2):   0%|          | 0/395 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=2):  50%|█████     | 198/395 [00:00<00:00, 604.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=2): 100%|██████████| 395/395 [00:00<00:00, 838.39 examples/s]
Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!
Unsloth: Tokenizing ["text"] (num_proc=2):   0%|          | 0/44 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=2):  50%|█████     | 22/44 [00:00<00:00, 89.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=2): 100%|██████████| 44/44 [00:00<00:00, 125.65 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 395 | Num Epochs = 3 | Total steps = 147
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 41,943,040/7,000,000,000 (0.60% trained)
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/coder/project/yunus/wandb/run-20250522_131919-lkq5n4pt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run yunus_mistral_finetune
wandb: ⭐️ View project at https://wandb.ai/tijaniyunus07-constructor-institute/huggingface
wandb: 🚀 View run at https://wandb.ai/tijaniyunus07-constructor-institute/huggingface/runs/lkq5n4pt
Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!
GPU = NVIDIA A100 80GB PCIe. Max memory = 79.138 GB.
4.051 GB of memory reserved.
  0%|          | 0/147 [00:00<?, ?it/s]  1%|          | 1/147 [00:05<12:41,  5.22s/it]  1%|▏         | 2/147 [00:06<07:33,  3.13s/it]  2%|▏         | 3/147 [00:08<06:21,  2.65s/it]  3%|▎         | 4/147 [00:10<05:32,  2.33s/it]  3%|▎         | 5/147 [00:12<05:03,  2.14s/it]                                                 3%|▎         | 5/147 [00:12<05:03,  2.14s/it]  4%|▍         | 6/147 [00:14<04:37,  1.97s/it]  5%|▍         | 7/147 [00:16<04:30,  1.93s/it]  5%|▌         | 8/147 [00:17<04:21,  1.88s/it]  6%|▌         | 9/147 [00:19<04:16,  1.86s/it]  7%|▋         | 10/147 [00:21<04:10,  1.83s/it]                                                  7%|▋         | 10/147 [00:21<04:10,  1.83s/it]  7%|▋         | 11/147 [00:23<04:35,  2.03s/it]  8%|▊         | 12/147 [00:25<04:24,  1.96s/it]  9%|▉         | 13/147 [00:27<04:16,  1.91s/it] 10%|▉         | 14/147 [00:29<04:08,  1.87s/it] 10%|█         | 15/147 [00:31<04:17,  1.95s/it]                                                 10%|█         | 15/147 [00:31<04:17,  1.95s/it] 11%|█         | 16/147 [00:33<04:04,  1.86s/it] 12%|█▏        | 17/147 [00:35<04:07,  1.90s/it] 12%|█▏        | 18/147 [00:36<04:00,  1.87s/it] 13%|█▎        | 19/147 [00:38<04:05,  1.92s/it] 14%|█▎        | 20/147 [00:40<03:56,  1.86s/it]                                                 14%|█▎        | 20/147 [00:40<03:56,  1.86s/it] 14%|█▍        | 21/147 [00:42<03:46,  1.80s/it] 15%|█▍        | 22/147 [00:44<03:44,  1.79s/it] 16%|█▌        | 23/147 [00:45<03:45,  1.81s/it] 16%|█▋        | 24/147 [00:47<03:45,  1.83s/it] 17%|█▋        | 25/147 [00:49<03:50,  1.89s/it]                                                 17%|█▋        | 25/147 [00:49<03:50,  1.89s/it]Unsloth: Not an error, but MistralForCausalLM does not accept `num_items_in_batch`.
Using gradient accumulation will be very slightly less accurate.
Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 2.5454, 'grad_norm': 4.8815789222717285, 'learning_rate': 2e-05, 'epoch': 0.1}
{'loss': 2.2727, 'grad_norm': 3.7006466388702393, 'learning_rate': 1.9295774647887327e-05, 'epoch': 0.2}
{'loss': 1.8857, 'grad_norm': 1.4449416399002075, 'learning_rate': 1.859154929577465e-05, 'epoch': 0.3}
{'loss': 1.815, 'grad_norm': 1.6526539325714111, 'learning_rate': 1.7887323943661975e-05, 'epoch': 0.4}
{'loss': 1.7757, 'grad_norm': 2.7919046878814697, 'learning_rate': 1.7183098591549297e-05, 'epoch': 0.51}

  0%|          | 0/22 [00:00<?, ?it/s][A
 14%|█▎        | 3/22 [00:00<00:01, 15.73it/s][A
 23%|██▎       | 5/22 [00:00<00:01, 10.60it/s][A
 32%|███▏      | 7/22 [00:00<00:01,  9.03it/s][A
 36%|███▋      | 8/22 [00:01<00:02,  6.34it/s][A
 45%|████▌     | 10/22 [00:01<00:01,  7.46it/s][A
 55%|█████▍    | 12/22 [00:01<00:01,  8.35it/s][A
 59%|█████▉    | 13/22 [00:01<00:01,  8.54it/s][A
 64%|██████▎   | 14/22 [00:01<00:01,  7.23it/s][A
 68%|██████▊   | 15/22 [00:01<00:00,  7.68it/s][A
 77%|███████▋  | 17/22 [00:02<00:00,  8.55it/s][A
 86%|████████▋ | 19/22 [00:02<00:00,  9.30it/s][A
 95%|█████████▌| 21/22 [00:02<00:00,  9.67it/s][A                                                
                                               [A 17%|█▋        | 25/147 [00:52<03:50,  1.89s/it]
100%|██████████| 22/22 [00:02<00:00,  9.67it/s][A
                                               [A 18%|█▊        | 26/147 [00:54<05:32,  2.75s/it] 18%|█▊        | 27/147 [00:56<04:55,  2.46s/it] 19%|█▉        | 28/147 [00:58<04:23,  2.22s/it] 20%|█▉        | 29/147 [00:59<04:00,  2.04s/it] 20%|██        | 30/147 [01:01<03:58,  2.04s/it]                                                 20%|██        | 30/147 [01:01<03:58,  2.04s/it] 21%|██        | 31/147 [01:03<03:46,  1.95s/it] 22%|██▏       | 32/147 [01:05<03:49,  1.99s/it] 22%|██▏       | 33/147 [01:07<03:46,  1.99s/it] 23%|██▎       | 34/147 [01:09<03:31,  1.87s/it] 24%|██▍       | 35/147 [01:10<03:24,  1.82s/it]                                                 24%|██▍       | 35/147 [01:10<03:24,  1.82s/it] 24%|██▍       | 36/147 [01:12<03:16,  1.77s/it] 25%|██▌       | 37/147 [01:14<03:12,  1.75s/it] 26%|██▌       | 38/147 [01:15<03:13,  1.77s/it] 27%|██▋       | 39/147 [01:17<03:09,  1.76s/it] 27%|██▋       | 40/147 [01:19<03:05,  1.73s/it]                                                 27%|██▋       | 40/147 [01:19<03:05,  1.73s/it] 28%|██▊       | 41/147 [01:21<03:05,  1.75s/it] 29%|██▊       | 42/147 [01:23<03:08,  1.80s/it] 29%|██▉       | 43/147 [01:24<03:10,  1.83s/it] 30%|██▉       | 44/147 [01:26<03:02,  1.77s/it] 31%|███       | 45/147 [01:28<02:58,  1.75s/it]                                                 31%|███       | 45/147 [01:28<02:58,  1.75s/it] 31%|███▏      | 46/147 [01:30<02:59,  1.78s/it] 32%|███▏      | 47/147 [01:32<03:06,  1.87s/it] 33%|███▎      | 48/147 [01:34<03:11,  1.93s/it] 33%|███▎      | 49/147 [01:36<03:07,  1.92s/it] 34%|███▍      | 50/147 [01:37<02:39,  1.65s/it]                                                 34%|███▍      | 50/147 [01:37<02:39,  1.65s/it]{'eval_loss': 1.8480457067489624, 'eval_runtime': 2.7152, 'eval_samples_per_second': 16.205, 'eval_steps_per_second': 8.103, 'epoch': 0.51}
{'loss': 1.7992, 'grad_norm': 0.8783057928085327, 'learning_rate': 1.6478873239436623e-05, 'epoch': 0.61}
{'loss': 1.5576, 'grad_norm': 1.1329151391983032, 'learning_rate': 1.5774647887323945e-05, 'epoch': 0.71}
{'loss': 1.5785, 'grad_norm': 1.2565586566925049, 'learning_rate': 1.5070422535211269e-05, 'epoch': 0.81}
{'loss': 1.5102, 'grad_norm': 1.6008832454681396, 'learning_rate': 1.4366197183098594e-05, 'epoch': 0.91}
{'loss': 1.3799, 'grad_norm': 1.1351287364959717, 'learning_rate': 1.3661971830985916e-05, 'epoch': 1.0}

  0%|          | 0/22 [00:00<?, ?it/s][A
  9%|▉         | 2/22 [00:00<00:01, 15.76it/s][A
 18%|█▊        | 4/22 [00:00<00:01, 11.90it/s][A
 27%|██▋       | 6/22 [00:00<00:01,  8.28it/s][A
 36%|███▋      | 8/22 [00:01<00:02,  6.44it/s][A
 41%|████      | 9/22 [00:01<00:01,  6.93it/s][A
 50%|█████     | 11/22 [00:01<00:01,  8.03it/s][A
 59%|█████▉    | 13/22 [00:01<00:01,  8.84it/s][A
 64%|██████▎   | 14/22 [00:01<00:01,  7.52it/s][A
 73%|███████▎  | 16/22 [00:01<00:00,  8.47it/s][A
 82%|████████▏ | 18/22 [00:02<00:00,  9.18it/s][A
 91%|█████████ | 20/22 [00:02<00:00,  9.62it/s][A
100%|██████████| 22/22 [00:02<00:00, 10.03it/s][A                                                
                                               [A 34%|███▍      | 50/147 [01:39<02:39,  1.65s/it]
100%|██████████| 22/22 [00:02<00:00, 10.03it/s][A
                                               [A 35%|███▍      | 51/147 [01:41<03:52,  2.42s/it] 35%|███▌      | 52/147 [01:43<03:38,  2.30s/it] 36%|███▌      | 53/147 [01:45<03:18,  2.11s/it] 37%|███▋      | 54/147 [01:47<03:10,  2.05s/it] 37%|███▋      | 55/147 [01:48<03:04,  2.00s/it]                                                 37%|███▋      | 55/147 [01:48<03:04,  2.00s/it] 38%|███▊      | 56/147 [01:50<02:56,  1.94s/it] 39%|███▉      | 57/147 [01:52<02:47,  1.86s/it] 39%|███▉      | 58/147 [01:54<02:50,  1.92s/it] 40%|████      | 59/147 [01:56<02:41,  1.83s/it] 41%|████      | 60/147 [01:58<02:46,  1.91s/it]                                                 41%|████      | 60/147 [01:58<02:46,  1.91s/it] 41%|████▏     | 61/147 [02:00<02:44,  1.91s/it] 42%|████▏     | 62/147 [02:01<02:36,  1.84s/it] 43%|████▎     | 63/147 [02:03<02:31,  1.81s/it] 44%|████▎     | 64/147 [02:05<02:36,  1.88s/it] 44%|████▍     | 65/147 [02:07<02:37,  1.93s/it]                                                 44%|████▍     | 65/147 [02:07<02:37,  1.93s/it] 45%|████▍     | 66/147 [02:09<02:38,  1.96s/it] 46%|████▌     | 67/147 [02:11<02:30,  1.88s/it] 46%|████▋     | 68/147 [02:13<02:24,  1.83s/it] 47%|████▋     | 69/147 [02:14<02:19,  1.79s/it] 48%|████▊     | 70/147 [02:16<02:20,  1.82s/it]                                                 48%|████▊     | 70/147 [02:16<02:20,  1.82s/it] 48%|████▊     | 71/147 [02:18<02:20,  1.85s/it] 49%|████▉     | 72/147 [02:20<02:23,  1.91s/it] 50%|████▉     | 73/147 [02:22<02:18,  1.87s/it] 50%|█████     | 74/147 [02:24<02:12,  1.81s/it] 51%|█████     | 75/147 [02:25<02:10,  1.81s/it]                                                 51%|█████     | 75/147 [02:25<02:10,  1.81s/it]{'eval_loss': 1.5215229988098145, 'eval_runtime': 2.6028, 'eval_samples_per_second': 16.905, 'eval_steps_per_second': 8.452, 'epoch': 1.0}
{'loss': 1.4245, 'grad_norm': 0.8941512703895569, 'learning_rate': 1.2957746478873242e-05, 'epoch': 1.1}
{'loss': 1.324, 'grad_norm': 0.7070820927619934, 'learning_rate': 1.2253521126760564e-05, 'epoch': 1.2}
{'loss': 1.3537, 'grad_norm': 0.8621351718902588, 'learning_rate': 1.1549295774647888e-05, 'epoch': 1.3}
{'loss': 1.3852, 'grad_norm': 0.956088125705719, 'learning_rate': 1.0845070422535213e-05, 'epoch': 1.4}
{'loss': 1.4003, 'grad_norm': 0.987789511680603, 'learning_rate': 1.0140845070422535e-05, 'epoch': 1.51}

  0%|          | 0/22 [00:00<?, ?it/s][A
  9%|▉         | 2/22 [00:00<00:01, 19.57it/s][A
 18%|█▊        | 4/22 [00:00<00:01, 12.27it/s][A
 27%|██▋       | 6/22 [00:00<00:01,  8.52it/s][A
 36%|███▋      | 8/22 [00:01<00:02,  6.54it/s][A
 41%|████      | 9/22 [00:01<00:01,  7.07it/s][A
 45%|████▌     | 10/22 [00:01<00:01,  7.45it/s][A
 50%|█████     | 11/22 [00:01<00:01,  7.83it/s][A
 59%|█████▉    | 13/22 [00:01<00:01,  8.78it/s][A
 64%|██████▎   | 14/22 [00:01<00:01,  7.35it/s][A
 73%|███████▎  | 16/22 [00:01<00:00,  8.46it/s][A
 82%|████████▏ | 18/22 [00:02<00:00,  9.14it/s][A
 91%|█████████ | 20/22 [00:02<00:00,  9.65it/s][A
100%|██████████| 22/22 [00:02<00:00, 10.06it/s][A                                                
                                               [A 51%|█████     | 75/147 [02:28<02:10,  1.81s/it]
100%|██████████| 22/22 [00:02<00:00, 10.06it/s][A
                                               [A 52%|█████▏    | 76/147 [02:30<03:08,  2.65s/it] 52%|█████▏    | 77/147 [02:32<02:46,  2.38s/it] 53%|█████▎    | 78/147 [02:33<02:29,  2.16s/it] 54%|█████▎    | 79/147 [02:35<02:20,  2.06s/it] 54%|█████▍    | 80/147 [02:37<02:13,  1.99s/it]                                                 54%|█████▍    | 80/147 [02:37<02:13,  1.99s/it] 55%|█████▌    | 81/147 [02:39<02:10,  1.98s/it] 56%|█████▌    | 82/147 [02:41<02:08,  1.98s/it] 56%|█████▋    | 83/147 [02:43<02:03,  1.94s/it] 57%|█████▋    | 84/147 [02:45<01:58,  1.88s/it] 58%|█████▊    | 85/147 [02:46<01:54,  1.84s/it]                                                 58%|█████▊    | 85/147 [02:46<01:54,  1.84s/it] 59%|█████▊    | 86/147 [02:48<01:55,  1.89s/it] 59%|█████▉    | 87/147 [02:50<01:54,  1.90s/it] 60%|█████▉    | 88/147 [02:52<01:54,  1.95s/it] 61%|██████    | 89/147 [02:54<01:48,  1.88s/it] 61%|██████    | 90/147 [02:56<01:48,  1.90s/it]                                                 61%|██████    | 90/147 [02:56<01:48,  1.90s/it] 62%|██████▏   | 91/147 [02:58<01:44,  1.87s/it] 63%|██████▎   | 92/147 [02:59<01:40,  1.82s/it] 63%|██████▎   | 93/147 [03:01<01:37,  1.80s/it] 64%|██████▍   | 94/147 [03:03<01:33,  1.76s/it] 65%|██████▍   | 95/147 [03:05<01:31,  1.76s/it]                                                 65%|██████▍   | 95/147 [03:05<01:31,  1.76s/it] 65%|██████▌   | 96/147 [03:06<01:28,  1.73s/it] 66%|██████▌   | 97/147 [03:08<01:26,  1.72s/it] 67%|██████▋   | 98/147 [03:10<01:24,  1.73s/it] 67%|██████▋   | 99/147 [03:11<01:23,  1.73s/it] 68%|██████▊   | 100/147 [03:12<01:09,  1.48s/it]                                                  68%|██████▊   | 100/147 [03:12<01:09,  1.48s/it]{'eval_loss': 1.45403254032135, 'eval_runtime': 2.6114, 'eval_samples_per_second': 16.849, 'eval_steps_per_second': 8.424, 'epoch': 1.51}
{'loss': 1.4851, 'grad_norm': 1.7481729984283447, 'learning_rate': 9.43661971830986e-06, 'epoch': 1.61}
{'loss': 1.3884, 'grad_norm': 0.9996197819709778, 'learning_rate': 8.732394366197183e-06, 'epoch': 1.71}
{'loss': 1.2967, 'grad_norm': 0.8016268610954285, 'learning_rate': 8.028169014084509e-06, 'epoch': 1.81}
{'loss': 1.4352, 'grad_norm': 0.9769966006278992, 'learning_rate': 7.3239436619718316e-06, 'epoch': 1.91}
{'loss': 1.418, 'grad_norm': 3.0732574462890625, 'learning_rate': 6.619718309859155e-06, 'epoch': 2.0}

  0%|          | 0/22 [00:00<?, ?it/s][A
 14%|█▎        | 3/22 [00:00<00:01, 15.97it/s][A
 23%|██▎       | 5/22 [00:00<00:01, 10.71it/s][A
 32%|███▏      | 7/22 [00:00<00:01,  9.15it/s][A
 36%|███▋      | 8/22 [00:01<00:02,  6.36it/s][A
 41%|████      | 9/22 [00:01<00:01,  6.87it/s][A
 45%|████▌     | 10/22 [00:01<00:01,  7.42it/s][A
 55%|█████▍    | 12/22 [00:01<00:01,  8.63it/s][A
 64%|██████▎   | 14/22 [00:01<00:01,  7.76it/s][A
 68%|██████▊   | 15/22 [00:01<00:00,  8.13it/s][A
 77%|███████▋  | 17/22 [00:02<00:00,  8.99it/s][A
 86%|████████▋ | 19/22 [00:02<00:00,  9.63it/s][A
 95%|█████████▌| 21/22 [00:02<00:00, 10.11it/s][A                                                 
                                               [A 68%|██████▊   | 100/147 [03:15<01:09,  1.48s/it]
100%|██████████| 22/22 [00:02<00:00, 10.11it/s][A
                                               [A 69%|██████▊   | 101/147 [03:17<01:45,  2.30s/it] 69%|██████▉   | 102/147 [03:18<01:34,  2.10s/it] 70%|███████   | 103/147 [03:20<01:27,  1.99s/it] 71%|███████   | 104/147 [03:22<01:21,  1.89s/it] 71%|███████▏  | 105/147 [03:23<01:19,  1.90s/it]                                                  71%|███████▏  | 105/147 [03:23<01:19,  1.90s/it] 72%|███████▏  | 106/147 [03:26<01:19,  1.93s/it] 73%|███████▎  | 107/147 [03:27<01:15,  1.89s/it] 73%|███████▎  | 108/147 [03:29<01:11,  1.82s/it] 74%|███████▍  | 109/147 [03:31<01:07,  1.78s/it] 75%|███████▍  | 110/147 [03:33<01:10,  1.91s/it]                                                  75%|███████▍  | 110/147 [03:33<01:10,  1.91s/it] 76%|███████▌  | 111/147 [03:35<01:11,  1.99s/it] 76%|███████▌  | 112/147 [03:37<01:06,  1.89s/it] 77%|███████▋  | 113/147 [03:39<01:07,  1.97s/it] 78%|███████▊  | 114/147 [03:41<01:05,  1.99s/it] 78%|███████▊  | 115/147 [03:43<01:04,  2.01s/it]                                                  78%|███████▊  | 115/147 [03:43<01:04,  2.01s/it] 79%|███████▉  | 116/147 [03:45<01:00,  1.94s/it] 80%|███████▉  | 117/147 [03:47<00:57,  1.92s/it] 80%|████████  | 118/147 [03:49<00:57,  1.99s/it] 81%|████████  | 119/147 [03:51<00:55,  1.99s/it] 82%|████████▏ | 120/147 [03:52<00:51,  1.90s/it]                                                  82%|████████▏ | 120/147 [03:52<00:51,  1.90s/it] 82%|████████▏ | 121/147 [03:54<00:48,  1.85s/it] 83%|████████▎ | 122/147 [03:56<00:45,  1.81s/it] 84%|████████▎ | 123/147 [03:58<00:42,  1.76s/it] 84%|████████▍ | 124/147 [03:59<00:39,  1.73s/it] 85%|████████▌ | 125/147 [04:01<00:37,  1.70s/it]                                                  85%|████████▌ | 125/147 [04:01<00:37,  1.70s/it]{'eval_loss': 1.4280333518981934, 'eval_runtime': 2.5594, 'eval_samples_per_second': 17.192, 'eval_steps_per_second': 8.596, 'epoch': 2.0}
{'loss': 1.3221, 'grad_norm': 1.1539371013641357, 'learning_rate': 5.915492957746479e-06, 'epoch': 2.1}
{'loss': 1.341, 'grad_norm': 0.914004385471344, 'learning_rate': 5.211267605633803e-06, 'epoch': 2.2}
{'loss': 1.2187, 'grad_norm': 0.910385012626648, 'learning_rate': 4.507042253521127e-06, 'epoch': 2.3}
{'loss': 1.4128, 'grad_norm': 2.0779807567596436, 'learning_rate': 3.8028169014084508e-06, 'epoch': 2.4}
{'loss': 1.3574, 'grad_norm': 1.2872402667999268, 'learning_rate': 3.0985915492957746e-06, 'epoch': 2.51}

  0%|          | 0/22 [00:00<?, ?it/s][A
 14%|█▎        | 3/22 [00:00<00:01, 15.23it/s][A
 23%|██▎       | 5/22 [00:00<00:01, 10.54it/s][A
 32%|███▏      | 7/22 [00:00<00:01,  8.57it/s][A
 36%|███▋      | 8/22 [00:01<00:02,  6.15it/s][A
 41%|████      | 9/22 [00:01<00:01,  6.74it/s][A
 45%|████▌     | 10/22 [00:01<00:01,  7.21it/s][A
 50%|█████     | 11/22 [00:01<00:01,  7.50it/s][A
 59%|█████▉    | 13/22 [00:01<00:01,  8.45it/s][A
 64%|██████▎   | 14/22 [00:01<00:01,  7.14it/s][A
 73%|███████▎  | 16/22 [00:01<00:00,  8.33it/s][A
 82%|████████▏ | 18/22 [00:02<00:00,  9.09it/s][A
 91%|█████████ | 20/22 [00:02<00:00,  9.66it/s][A
100%|██████████| 22/22 [00:02<00:00, 10.04it/s][A                                                 
                                               [A 85%|████████▌ | 125/147 [04:03<00:37,  1.70s/it]
100%|██████████| 22/22 [00:02<00:00, 10.04it/s][A
                                               [A 86%|████████▌ | 126/147 [04:05<00:52,  2.52s/it] 86%|████████▋ | 127/147 [04:07<00:45,  2.27s/it] 87%|████████▋ | 128/147 [04:09<00:39,  2.10s/it] 88%|████████▊ | 129/147 [04:10<00:35,  1.98s/it] 88%|████████▊ | 130/147 [04:12<00:33,  1.98s/it]                                                  88%|████████▊ | 130/147 [04:12<00:33,  1.98s/it] 89%|████████▉ | 131/147 [04:14<00:30,  1.91s/it] 90%|████████▉ | 132/147 [04:16<00:27,  1.85s/it] 90%|█████████ | 133/147 [04:18<00:26,  1.89s/it] 91%|█████████ | 134/147 [04:20<00:24,  1.89s/it] 92%|█████████▏| 135/147 [04:22<00:22,  1.89s/it]                                                  92%|█████████▏| 135/147 [04:22<00:22,  1.89s/it] 93%|█████████▎| 136/147 [04:23<00:20,  1.85s/it] 93%|█████████▎| 137/147 [04:25<00:19,  1.90s/it] 94%|█████████▍| 138/147 [04:27<00:16,  1.84s/it] 95%|█████████▍| 139/147 [04:29<00:14,  1.86s/it] 95%|█████████▌| 140/147 [04:31<00:12,  1.81s/it]                                                  95%|█████████▌| 140/147 [04:31<00:12,  1.81s/it] 96%|█████████▌| 141/147 [04:32<00:10,  1.81s/it] 97%|█████████▋| 142/147 [04:34<00:09,  1.85s/it] 97%|█████████▋| 143/147 [04:36<00:07,  1.82s/it] 98%|█████████▊| 144/147 [04:38<00:05,  1.89s/it] 99%|█████████▊| 145/147 [04:40<00:03,  1.89s/it]                                                  99%|█████████▊| 145/147 [04:40<00:03,  1.89s/it] 99%|█████████▉| 146/147 [04:42<00:01,  1.84s/it]100%|██████████| 147/147 [04:43<00:00,  1.80s/it]                                                 100%|██████████| 147/147 [04:44<00:00,  1.80s/it]100%|██████████| 147/147 [04:44<00:00,  1.94s/it]
Unsloth: Will map <|im_end|> to EOS = <|im_end|>.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'eval_loss': 1.4162631034851074, 'eval_runtime': 2.6555, 'eval_samples_per_second': 16.57, 'eval_steps_per_second': 8.285, 'epoch': 2.51}
{'loss': 1.3763, 'grad_norm': 1.180169701576233, 'learning_rate': 2.3943661971830984e-06, 'epoch': 2.61}
{'loss': 1.2684, 'grad_norm': 1.0691075325012207, 'learning_rate': 1.6901408450704227e-06, 'epoch': 2.71}
{'loss': 1.3155, 'grad_norm': 1.7968580722808838, 'learning_rate': 9.859154929577465e-07, 'epoch': 2.81}
{'loss': 1.2915, 'grad_norm': 1.0425403118133545, 'learning_rate': 2.8169014084507043e-07, 'epoch': 2.91}
{'train_runtime': 285.7394, 'train_samples_per_second': 4.147, 'train_steps_per_second': 0.514, 'train_loss': 1.513043650153543, 'epoch': 2.95}
285.7394 seconds used for training.
4.76 minutes used for training.
Peak reserved memory = 5.588 GB.
Peak reserved memory for training = 1.537 GB.
Peak reserved memory % of max memory = 7.061 %.
Peak reserved memory for training % of max memory = 1.942 %.
<|im_start|>user
Who is the lecturer of capstone course<|im_end|>
<|im_start|>assistant
The lecturer of the capstone course is Dr. X.<|im_end|>
<|im_start|>user
What are the previous projects done in Capstone course?<|im_end|>
<|im_start|>assistant
The Capstone course involves working on a project from start to finish, including planning, design, implementation, and presentation. Past projects have included a variety of software systems, such as a web-based platform for managing and tracking student projects, a mobile app for managing personal finances, and a system for managing and tracking student attendance.<|im_end|>
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33myunus_mistral_finetune[0m at: [34mhttps://wandb.ai/tijaniyunus07-constructor-institute/huggingface/runs/lkq5n4pt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250522_131919-lkq5n4pt/logs[0m
